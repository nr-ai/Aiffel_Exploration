{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 붓꽃 분류"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 로드"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 머신러닝 라이브러리 중 하나인 사이킷런(scikit-learn)에서 제공되는 데이터셋 사용\n",
    "- pandas : 표 형태의 2차원 배열 데이터 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>6.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
       "0                  5.1               3.5                1.4               0.2   \n",
       "1                  4.9               3.0                1.4               0.2   \n",
       "2                  4.7               3.2                1.3               0.2   \n",
       "3                  4.6               3.1                1.5               0.2   \n",
       "4                  5.0               3.6                1.4               0.2   \n",
       "..                 ...               ...                ...               ...   \n",
       "145                6.7               3.0                5.2               2.3   \n",
       "146                6.3               2.5                5.0               1.9   \n",
       "147                6.5               3.0                5.2               2.0   \n",
       "148                6.2               3.4                5.4               2.3   \n",
       "149                5.9               3.0                5.1               1.8   \n",
       "\n",
       "     label  \n",
       "0        0  \n",
       "1        0  \n",
       "2        0  \n",
       "3        0  \n",
       "4        0  \n",
       "..     ...  \n",
       "145      2  \n",
       "146      2  \n",
       "147      2  \n",
       "148      2  \n",
       "149      2  \n",
       "\n",
       "[150 rows x 5 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "\n",
    "import pandas as pd\n",
    "iris_df = pd.DataFrame(data=iris.data, columns = iris.feature_names)\n",
    "iris_df[\"label\"] = iris.target\n",
    "iris_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터셋을 train dataset, test dataset 으로 나누기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set 개수: 120 , test set 개수: 30\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "iris_data = iris.data\n",
    "iris_label = iris.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris_data, iris_label, test_size = 0.2, random_state=25)\n",
    "print(\"training set 개수:\", len(X_train), \", test set 개수:\", len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 머신러닝 모델링 (Decision Tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[blog](https://ratsgo.github.io/machine%20learning/2017/03/26/tree/)\n",
    "- decision 기준을 결정할 때, X(속성 데이터)의 모든 속성에 대해서 다음을 수행함\n",
    "    - sorting 한 후, value를 오름차순으로 차례대로 기준으로 삼음\n",
    "    - 각 기준일 때의 entropy 를 계산함\n",
    "    - entropy 가 가장 낮을 때를 decision 기준으로 결정함\n",
    "- 모든 경우의 수 가운데 정보획득이 가장 큰 변수와 그 지점에서 첫번째 분기를 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "decision_tree = DecisionTreeClassifier()\n",
    "#decision_tree = DecisionTreeClassifier(random_state=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier()"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decision_tree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 1, 2, 1, 2, 0, 1, 1, 0, 0, 0, 1, 0, 1, 2, 2, 1, 1, 1, 1,\n",
       "       1, 0, 0, 2, 1, 2, 2, 0])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = decision_tree.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 정확도 확인하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         9\n",
      "           1       0.92      0.92      0.92        13\n",
      "           2       0.88      0.88      0.88         8\n",
      "\n",
      "    accuracy                           0.93        30\n",
      "   macro avg       0.93      0.93      0.93        30\n",
      "weighted avg       0.93      0.93      0.93        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "accuracy\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 정확도 높이기\n",
    "[blog](https://medium.com/@deepvalidation/title-3b0e263605de)\n",
    "- decision 기준을 결정할 때, X(속성 데이터)의 모든 속성이 아니라, 랜덤으로 뽑은 몇 가지 속성에 대해서 decision tree 만드는 과정을 수행함\n",
    "- 여러 decision tree 모델이 나와서 Ensemble 기법으로 평균 낸 값을 결과로 함\n",
    "- 전체 데이터에서 가장 영향력이 큰 요소만 포함했다면 탈락했을 수도 있는 less major factors도 살펴볼 수 있다는 점에서 overfitting 을 줄일 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         9\n",
      "           1       0.92      0.92      0.92        13\n",
      "           2       0.88      0.88      0.88         8\n",
      "\n",
      "    accuracy                           0.93        30\n",
      "   macro avg       0.93      0.93      0.93        30\n",
      "weighted avg       0.93      0.93      0.93        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris_data, \n",
    "                                                    iris_label, \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=25)\n",
    "random_forest = RandomForestClassifier()\n",
    "random_forest.fit(X_train,y_train)\n",
    "y_pred_rf = random_forest.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "정확도를 높이기 위해 random forest 를 썼지만, 오히려 성능이 안 좋아졌다. 사소한 요소를 전부 보는 것보다는 main factor가 결과 예측에 더 중요한 역할을 한다는 것을 추론할 수 있다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 그 외 데이터 마이닝 모델\n",
    "- Support Vector Machine (SVM)\n",
    "- SGD Classifier\n",
    "- Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM\n",
    "[blog1](https://excelsior-cjh.tistory.com/66?category=918734)\n",
    "[blog2](http://jaejunyoo.blogspot.com/2018/01/support-vector-machine-1.html)\n",
    "[MIT OpenCourseWare](https://www.youtube.com/watch?v=_PwhiWxHK8o&t=6s)\n",
    "\n",
    "### 1. SVM 이해\n",
    "![widest street strategy](https://4.bp.blogspot.com/-pYY6U0peR74/WmwCRbFsM7I/AAAAAAAACfQ/gwcY8_j13yU6qwkITBmgd2yrVkKd2hvQwCK4BGAYYCw/s200/svm2.png)\n",
    "decision boundraries를 그린다면, 서로 다른 class를 가진 데이터들과 decision boundaries의 거리가 멀면 멀수록 좋다. 가까우면, classification 정확도가 떨어진다. (=widest street strategy)\n",
    "\n",
    "### 2. Decision rule\n",
    "![decision rule](https://1.bp.blogspot.com/-n2tSWYD_XoA/WmwE1cP5ctI/AAAAAAAACfo/3eniT7SzJioB4SdJVL-HTJSb_ouTLLQ_gCK4BGAYYCw/s200/svm3.png)\n",
    "- decision boundary 의 중심선에 대해 직교하는 벡터 w가 있다.\n",
    "- 우리가 분류하고 싶은 데이터 샘플 하나인 벡터 u가 있다.\n",
    "- u를 w에 내적하면(projection) w 방향(decision boundary 중심선에 직교한 방향)으로 벡터가 그어지는데, 그 결과는 u 샘플의 정보를 길이로 담는다.\n",
    "- 그 결과가 decision boundary 의 중심선을 넘는지 넘지 않는지로, 분류할 수 있다.\n",
    "- 그러고 보면, 직교한 벡터를 활용한 이유가, decision boundary 의 중심선을 지나는지 아닌지로 분류하고 싶어서 쓴 것임을 알 수 있다.\n",
    "\n",
    "### 3. Support Vector\n",
    "- decision boundary 의 양쪽 경계선에 딱 만나는 (경계선 위에 있는) 서로 다른 클래스의 두 벡터 사이의 거리를 구할 수 있다. = 2/|w|\n",
    "- 그 거리가 최대화하는 방향으로 계산을 하여 w 벡터를 구할 수 있다. 결과만 말하자면, 경계선 위의 샘플들의 선형 합으로 나타낼 수 있다. = max 2/|w| <=> min |w|\n",
    "- 경계선 위의 샘플들, 경계선을 정하는 샘플들을 \"support vector\"라고 한다.\n",
    "\n",
    "### 4. 소프트 마진\n",
    "- (위의 SVM은 하드마진), 서포트벡터의 경계선에 약간의 여유 변수를 더해줌\n",
    "\n",
    "### 5. 적용\n",
    "![mapping](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=http%3A%2F%2Fcfile29.uf.tistory.com%2Fimage%2F246E9C3A5852B489311F1D)\n",
    "- non-linearly separable (선형 분리가 불능한) 데이터를 충분히 큰 차원으로 적절한 비선형 매핑을 하면, 고차원에서는 linearly separable (선형 분리 가능한) 할 수 있다.\n",
    "- 따라서, 복잡한 비선형 의사결정 영역을 모형화 할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         9\n",
      "           1       1.00      0.92      0.96        13\n",
      "           2       0.89      1.00      0.94         8\n",
      "\n",
      "    accuracy                           0.97        30\n",
      "   macro avg       0.96      0.97      0.97        30\n",
      "weighted avg       0.97      0.97      0.97        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "svm_model = svm.SVC()\n",
    "svm_model.fit(X_train,y_train)\n",
    "y_pred_svm = svm_model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred_svm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD Classifier\n",
    "확률적 경사하강법 (SGD, Stochastic Gradient Descent)\n",
    "\n",
    "[blog](https://velog.io/@jihoson94/확률적-경사하강법-이해)\n",
    "- 모델을 학습시킬 때, loss function을 최소화하기 위해 loss function 의 기울기가 최소(혹은 0)이 되는 지점을 찾는 학습을 하였다.\n",
    "- 이 때, 전체 데이터의 loss function을 다 구하면서 계산을 하였는데 데이터셋 크기가 커질 수록 오랜 시간이 걸릴 수 있다.\n",
    "- SGD에서는 전체 데이터 대신 랜덤 데이터로 여러 번 반복해서 학습하면, 확률적으로 같은 결과가 나올 것이라는 접근을 하였다.\n",
    "- 랜덤하게 뽑은 하나의 데이터로 loss function을 구하고 최소화 하도록 학습하는 것을 여러 번 반복적으로 한다. 이 때 '하나'의 단위가 batch 이다.\n",
    "- 하나는 너무 적기 때문에 전체 데이터에서 n개의 데이터를 뽑은 후 학습을 진행하는 것으로 진화하였다. batch = n 으로 지정해주면 된다. 이를 mini-batch SGD 라고 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      1.00      0.69         9\n",
      "           1       1.00      0.08      0.14        13\n",
      "           2       0.67      1.00      0.80         8\n",
      "\n",
      "    accuracy                           0.60        30\n",
      "   macro avg       0.73      0.69      0.55        30\n",
      "weighted avg       0.77      0.60      0.48        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "sgd_model = SGDClassifier()\n",
    "sgd_model.fit(X_train, y_train)\n",
    "y_pred_sgd = sgd_model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred_sgd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 다른 모델을 쓰면서, optimize 할 때 SGD 를 써야 할 것 같은데, SGD를 classifier 로 썼기 때문에 정확도가 낮게 나온 것 같다. SGD만 단독으로 SGD classifer로 쓰면 어떤 의미를 가지는 지 모르겠다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "[blog](http://hleecaster.com/ml-logistic-regression-concept/)\n",
    "![logistic regression](https://i0.wp.com/hleecaster.com/wp-content/uploads/2019/12/logreg02.png?w=973)\n",
    "- Linear regression과 유사한 접근 방식이지만, linear regression은 확률이 음과 양의 무한대로 뻗어나간다. Logistic regression은 확률을 0과 1사이로 나오도록 sigmoid 함수처럼 만들어준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         9\n",
      "           1       1.00      0.92      0.96        13\n",
      "           2       0.89      1.00      0.94         8\n",
      "\n",
      "    accuracy                           0.97        30\n",
      "   macro avg       0.96      0.97      0.97        30\n",
      "weighted avg       0.97      0.97      0.97        30\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel0035/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logistic_model = LogisticRegression()\n",
    "logistic_model.fit(X_train, y_train)\n",
    "y_pred_log = logistic_model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred_log))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 성능 측정법\n",
    "\n",
    "[blog](https://manisha-sirsat.blogspot.com/2019/04/confusion-matrix.html)\n",
    "- 정확도 : 맞은 데이터만 신경쓰는 척도\n",
    "- 오차행렬 : 정답과 오답을 구분하여 평가\n",
    "![Confusion Matrix](https://2.bp.blogspot.com/-EvSXDotTOwc/XMfeOGZ-CVI/AAAAAAAAEiE/oePFfvhfOQM11dgRn9FkPxlegCXbgOF4QCLcBGAs/s1600/confusionMatrxiUpdated.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         9\n",
      "           1       0.92      0.92      0.92        13\n",
      "           2       0.88      0.88      0.88         8\n",
      "\n",
      "    accuracy                           0.93        30\n",
      "   macro avg       0.93      0.93      0.93        30\n",
      "weighted avg       0.93      0.93      0.93        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_test, y_pred)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 연습문제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 손글씨 분류 (load_digits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 데이터 준비하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target class: [0 1 2 3 4 5 6 7 8 9]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "digits = load_digits()\n",
    "digits_data = digits.data\n",
    "digits_label = digits.target\n",
    "\n",
    "print(\"target class:\", digits.target_names)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(digits_data, digits_label, test_size = 0.2, random_state =7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 모델 학습하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      1.00      0.97        43\n",
      "           1       0.75      0.86      0.80        42\n",
      "           2       0.84      0.78      0.81        40\n",
      "           3       0.91      0.91      0.91        34\n",
      "           4       0.85      0.95      0.90        37\n",
      "           5       0.90      0.96      0.93        28\n",
      "           6       0.93      0.93      0.93        28\n",
      "           7       0.90      0.79      0.84        33\n",
      "           8       0.87      0.63      0.73        43\n",
      "           9       0.75      0.84      0.79        32\n",
      "\n",
      "    accuracy                           0.86       360\n",
      "   macro avg       0.86      0.86      0.86       360\n",
      "weighted avg       0.86      0.86      0.86       360\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99        43\n",
      "           1       0.93      1.00      0.97        42\n",
      "           2       1.00      1.00      1.00        40\n",
      "           3       0.92      1.00      0.96        34\n",
      "           4       0.88      0.97      0.92        37\n",
      "           5       0.90      1.00      0.95        28\n",
      "           6       1.00      0.96      0.98        28\n",
      "           7       0.97      0.94      0.95        33\n",
      "           8       1.00      0.81      0.90        43\n",
      "           9       0.97      0.91      0.94        32\n",
      "\n",
      "    accuracy                           0.96       360\n",
      "   macro avg       0.96      0.96      0.96       360\n",
      "weighted avg       0.96      0.96      0.96       360\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        43\n",
      "           1       0.95      1.00      0.98        42\n",
      "           2       1.00      1.00      1.00        40\n",
      "           3       1.00      1.00      1.00        34\n",
      "           4       1.00      1.00      1.00        37\n",
      "           5       0.93      1.00      0.97        28\n",
      "           6       1.00      1.00      1.00        28\n",
      "           7       1.00      1.00      1.00        33\n",
      "           8       1.00      0.93      0.96        43\n",
      "           9       1.00      0.97      0.98        32\n",
      "\n",
      "    accuracy                           0.99       360\n",
      "   macro avg       0.99      0.99      0.99       360\n",
      "weighted avg       0.99      0.99      0.99       360\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        43\n",
      "           1       0.94      0.71      0.81        42\n",
      "           2       0.98      1.00      0.99        40\n",
      "           3       0.84      0.94      0.89        34\n",
      "           4       1.00      1.00      1.00        37\n",
      "           5       0.96      0.89      0.93        28\n",
      "           6       1.00      0.93      0.96        28\n",
      "           7       0.97      0.97      0.97        33\n",
      "           8       0.75      0.93      0.83        43\n",
      "           9       0.94      0.91      0.92        32\n",
      "\n",
      "    accuracy                           0.93       360\n",
      "   macro avg       0.94      0.93      0.93       360\n",
      "weighted avg       0.93      0.93      0.93       360\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        43\n",
      "           1       0.95      0.95      0.95        42\n",
      "           2       0.98      1.00      0.99        40\n",
      "           3       0.94      0.97      0.96        34\n",
      "           4       0.97      1.00      0.99        37\n",
      "           5       0.82      0.96      0.89        28\n",
      "           6       1.00      0.96      0.98        28\n",
      "           7       0.97      0.97      0.97        33\n",
      "           8       0.92      0.81      0.86        43\n",
      "           9       0.97      0.91      0.94        32\n",
      "\n",
      "    accuracy                           0.95       360\n",
      "   macro avg       0.95      0.95      0.95       360\n",
      "weighted avg       0.95      0.95      0.95       360\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel0035/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "# Decision Tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "DecisionTree = DecisionTreeClassifier()\n",
    "DecisionTree.fit(x_train, y_train)\n",
    "y_d = DecisionTree.predict(x_test)\n",
    "print(classification_report(y_test, y_d))\n",
    "\n",
    "# Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "RandomForest = RandomForestClassifier()\n",
    "RandomForest.fit(x_train, y_train)\n",
    "y_r = RandomForest.predict(x_test)\n",
    "print(classification_report(y_test, y_r))\n",
    "\n",
    "# SVM\n",
    "from sklearn import svm\n",
    "svm_model = svm.SVC()\n",
    "svm_model.fit(x_train, y_train)\n",
    "y_s = svm_model.predict(x_test)\n",
    "print(classification_report(y_test, y_s))\n",
    "\n",
    "# SGD Classifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "sgd_model = SGDClassifier()\n",
    "sgd_model.fit(x_train, y_train)\n",
    "y_sg = sgd_model.predict(x_test)\n",
    "print(classification_report(y_test, y_sg))\n",
    "\n",
    "# Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logiReg = LogisticRegression()\n",
    "logiReg.fit(x_train, y_train)\n",
    "y_l = logiReg.predict(x_test)\n",
    "print(classification_report(y_test, y_l))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
