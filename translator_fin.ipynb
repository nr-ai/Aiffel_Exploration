{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 단어 수준의 번역기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import shutil\n",
    "import os\n",
    "import unicodedata\n",
    "import urllib3\n",
    "import zipfile\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플의 수 : 178009\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng</th>\n",
       "      <th>fra</th>\n",
       "      <th>cc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>88866</th>\n",
       "      <td>You're not a very good liar.</td>\n",
       "      <td>Tu n'es pas une très bonne menteuse.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13630</th>\n",
       "      <td>All are welcomed.</td>\n",
       "      <td>Toutes sont bienvenues.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114899</th>\n",
       "      <td>Canned food doesn't interest her.</td>\n",
       "      <td>La nourriture en conserve ne l'intéresse pas.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113768</th>\n",
       "      <td>We managed to get there on time.</td>\n",
       "      <td>On est parvenu à s'y rendre à temps.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160526</th>\n",
       "      <td>They shook hands when they met at the airport.</td>\n",
       "      <td>Ils se serrèrent la main lorsqu'ils se rencont...</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   eng  \\\n",
       "88866                     You're not a very good liar.   \n",
       "13630                                All are welcomed.   \n",
       "114899               Canned food doesn't interest her.   \n",
       "113768                We managed to get there on time.   \n",
       "160526  They shook hands when they met at the airport.   \n",
       "\n",
       "                                                      fra  \\\n",
       "88866                Tu n'es pas une très bonne menteuse.   \n",
       "13630                             Toutes sont bienvenues.   \n",
       "114899      La nourriture en conserve ne l'intéresse pas.   \n",
       "113768               On est parvenu à s'y rendre à temps.   \n",
       "160526  Ils se serrèrent la main lorsqu'ils se rencont...   \n",
       "\n",
       "                                                       cc  \n",
       "88866   CC-BY 2.0 (France) Attribution: tatoeba.org #2...  \n",
       "13630   CC-BY 2.0 (France) Attribution: tatoeba.org #4...  \n",
       "114899  CC-BY 2.0 (France) Attribution: tatoeba.org #2...  \n",
       "113768  CC-BY 2.0 (France) Attribution: tatoeba.org #2...  \n",
       "160526  CC-BY 2.0 (France) Attribution: tatoeba.org #1...  "
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "file_path = os.getenv('HOME')+'/aiffel/translator_seq2seq/data/fra.txt'\n",
    "lines = pd.read_csv(file_path, names=['eng', 'fra', 'cc'], sep='\\t')\n",
    "print('전체 샘플의 수 :',len(lines))\n",
    "lines.sample(5) #샘플 5개 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng</th>\n",
       "      <th>fra</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>27733</th>\n",
       "      <td>You can't leave me.</td>\n",
       "      <td>Tu ne peux pas me quitter.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21326</th>\n",
       "      <td>They'll eat those.</td>\n",
       "      <td>Elles mangeront celles-là.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32527</th>\n",
       "      <td>Tom was my cellmate.</td>\n",
       "      <td>Tom était mon compagnon de cellule.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8638</th>\n",
       "      <td>Is it reliable?</td>\n",
       "      <td>Est-ce fiable ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21099</th>\n",
       "      <td>The bar is closed.</td>\n",
       "      <td>Le bar est fermé.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        eng                                  fra\n",
       "27733   You can't leave me.           Tu ne peux pas me quitter.\n",
       "21326    They'll eat those.           Elles mangeront celles-là.\n",
       "32527  Tom was my cellmate.  Tom était mon compagnon de cellule.\n",
       "8638        Is it reliable?                      Est-ce fiable ?\n",
       "21099    The bar is closed.                    Le bar est fermé."
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = lines[['eng', 'fra']][:33000] #33000개 샘플 사용\n",
    "lines.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unicode_to_ascii(s):\n",
    "  return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "      if unicodedata.category(c) != 'Mn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sent):\n",
    "    # 위에서 구현한 함수를 내부적으로 호출\n",
    "    sent = unicode_to_ascii(sent.lower())\n",
    "\n",
    "    # 단어와 구두점 사이에 공백을 만듭니다.\n",
    "    # Ex) \"he is a boy.\" => \"he is a boy .\"\n",
    "    sent = re.sub(r\"([?.!,¿])\", r\" \\1\", sent)\n",
    "\n",
    "    # (a-z, A-Z, \".\", \"?\", \"!\", \",\") 이들을 제외하고는 전부 공백으로 변환합니다.\n",
    "    sent = re.sub(r\"[^a-zA-Z!.?]+\", r\" \", sent)\n",
    "\n",
    "    sent = re.sub(r\"\\s+\", \" \", sent)\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "have you had dinner ?\n",
      "avez vous deja dine ?\n"
     ]
    }
   ],
   "source": [
    "en_sent = u\"Have you had dinner?\"\n",
    "fr_sent = u\"Avez-vous déjà diné?\"\n",
    "print(preprocess_sentence(en_sent))\n",
    "print(preprocess_sentence(fr_sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines.eng = lines.eng.apply(lambda x : preprocess_sentence(x))\n",
    "lines.fra = lines.fra.apply(lambda x : preprocess_sentence(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng</th>\n",
       "      <th>fra</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13735</th>\n",
       "      <td>attention please !</td>\n",
       "      <td>attention s il vous plait !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9143</th>\n",
       "      <td>tell everybody .</td>\n",
       "      <td>dis le a tout le monde .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25191</th>\n",
       "      <td>i ve seen too much .</td>\n",
       "      <td>j en ai trop vu .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4291</th>\n",
       "      <td>now drink up .</td>\n",
       "      <td>finissez votre verre maintenant !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2284</th>\n",
       "      <td>i like math .</td>\n",
       "      <td>j aime les maths .</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        eng                                fra\n",
       "13735    attention please !        attention s il vous plait !\n",
       "9143       tell everybody .           dis le a tout le monde .\n",
       "25191  i ve seen too much .                  j en ai trop vu .\n",
       "4291         now drink up .  finissez votre verre maintenant !\n",
       "2284          i like math .                 j aime les maths ."
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플의 수 : 33000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng</th>\n",
       "      <th>fra</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14680</th>\n",
       "      <td>i didn t mean it .</td>\n",
       "      <td>&lt;start&gt; ce n etait pas mon intention . &lt;end&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11064</th>\n",
       "      <td>i heard a noise .</td>\n",
       "      <td>&lt;start&gt; j ai entendu un bruit . &lt;end&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14284</th>\n",
       "      <td>he is very young .</td>\n",
       "      <td>&lt;start&gt; il est tres jeune . &lt;end&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17468</th>\n",
       "      <td>which is cheaper ?</td>\n",
       "      <td>&lt;start&gt; laquelle est la moins chere ? &lt;end&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8120</th>\n",
       "      <td>i like singing .</td>\n",
       "      <td>&lt;start&gt; j aime chanter . &lt;end&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      eng                                           fra\n",
       "14680  i didn t mean it .  <start> ce n etait pas mon intention . <end>\n",
       "11064   i heard a noise .         <start> j ai entendu un bruit . <end>\n",
       "14284  he is very young .             <start> il est tres jeune . <end>\n",
       "17468  which is cheaper ?   <start> laquelle est la moins chere ? <end>\n",
       "8120     i like singing .                <start> j aime chanter . <end>"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 시작 토큰과 종료 토큰 추가\n",
    "sos_token = '<start>'\n",
    "eos_token = '<end>'\n",
    "lines.fra = lines.fra.apply(lambda x : '<start> '+ x + ' <end>')\n",
    "print('전체 샘플의 수 :',len(lines))\n",
    "lines.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sentence):\n",
    "    tokenizer = Tokenizer(num_words=7000, filters=\"\", oov_token=\"<unk>\")\n",
    "    tokenizer.fit_on_texts(sentence)\n",
    "    tensor = tokenizer.texts_to_sequences(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[31, 2], [1133, 2], [1133, 2]]"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_tokenizer = Tokenizer(num_words=7000, filters=\"\", oov_token=\"<unk>\")   # 단어 단위로 Tokenizer를 생성합니다. \n",
    "eng_tokenizer.fit_on_texts(lines.eng)               # 33000개의 행을 가진 eng의 각 행에 토큰화를 수행\n",
    "input_text = eng_tokenizer.texts_to_sequences(lines.eng)    # 단어를 숫자값 인덱스로 변환하여 저장\n",
    "input_text[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2, 90, 13, 3], [2, 1021, 13, 3], [2, 1021, 4, 3]]"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fra_tokenizer = Tokenizer(num_words=7000, filters=\"\", oov_token=\"<unk>\")   # 단어 단위로 Tokenizer를 생성합니다. \n",
    "fra_tokenizer.fit_on_texts(lines.fra)                 # 33000개의 행을 가진 fra의 각 행에 토큰화를 수행\n",
    "target_text = fra_tokenizer.texts_to_sequences(lines.fra)     # 단어를 숫자값 인덱스로 변환하여 저장\n",
    "target_text[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : <unk>\n",
      "2 : .\n",
      "3 : i\n",
      "4 : you\n",
      "5 : ?\n",
      "6 : it\n",
      "7 : a\n",
      "8 : s\n",
      "9 : is\n",
      "10 : re\n"
     ]
    }
   ],
   "source": [
    "for idx in eng_tokenizer.index_word:\n",
    "    print(idx, \":\", eng_tokenizer.index_word[idx])\n",
    "\n",
    "    if idx >= 10: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : <unk>\n",
      "2 : <start>\n",
      "3 : <end>\n",
      "4 : .\n",
      "5 : je\n",
      "6 : est\n",
      "7 : ?\n",
      "8 : vous\n",
      "9 : a\n",
      "10 : pas\n"
     ]
    }
   ],
   "source": [
    "for idx in fra_tokenizer.index_word:\n",
    "    print(idx, \":\", fra_tokenizer.index_word[idx])\n",
    "\n",
    "    if idx >= 10: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 단어장의 크기 : 4664\n",
      "프랑스어 단어장의 크기 : 8039\n"
     ]
    }
   ],
   "source": [
    "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
    "fra_vocab_size = len(fra_tokenizer.word_index) + 1\n",
    "print('영어 단어장의 크기 :', eng_vocab_size)\n",
    "print('프랑스어 단어장의 크기 :', fra_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "fra_tokenizer.index_word[2] = '\\t'\n",
    "fra_tokenizer.index_word[3] = '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : <unk>\n",
      "2 : \t\n",
      "3 : \n",
      "\n",
      "4 : .\n",
      "5 : je\n",
      "6 : est\n",
      "7 : ?\n",
      "8 : vous\n",
      "9 : a\n",
      "10 : pas\n"
     ]
    }
   ],
   "source": [
    "for idx in fra_tokenizer.index_word:\n",
    "    print(idx, \":\", fra_tokenizer.index_word[idx])\n",
    "\n",
    "    if idx >= 10: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 시퀀스의 최대 길이 8\n",
      "프랑스어 시퀀스의 최대 길이 16\n"
     ]
    }
   ],
   "source": [
    "max_eng_seq_len = max([len(line) for line in encoder_input])\n",
    "max_fra_seq_len = max([len(line) for line in decoder_input])\n",
    "print('영어 시퀀스의 최대 길이', max_eng_seq_len)\n",
    "print('프랑스어 시퀀스의 최대 길이', max_fra_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플의 수 : 33000\n",
      "영어 단어장의 크기 : 4664\n",
      "프랑스어 단어장의 크기 : 8039\n",
      "영어 시퀀스의 최대 길이 8\n",
      "프랑스어 시퀀스의 최대 길이 16\n"
     ]
    }
   ],
   "source": [
    "print('전체 샘플의 수 :',len(lines))\n",
    "print('영어 단어장의 크기 :', eng_vocab_size)\n",
    "print('프랑스어 단어장의 크기 :', fra_vocab_size)\n",
    "print('영어 시퀀스의 최대 길이', max_eng_seq_len)\n",
    "print('프랑스어 시퀀스의 최대 길이', max_fra_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input = input_text\n",
    "# 종료 토큰 제거\n",
    "decoder_input = [[ char for char in line if char != fra_tokenizer.word_index[eos_token] ] for line in target_text] \n",
    "# 시작 토큰 제거\n",
    "decoder_target = [[ char for char in line if char != fra_tokenizer.word_index[sos_token] ] for line in target_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[31, 2], [1133, 2], [1133, 2]]\n",
      "[[2, 90, 13], [2, 1021, 13], [2, 1021, 4]]\n",
      "[[90, 13, 3], [1021, 13, 3], [1021, 4, 3]]\n"
     ]
    }
   ],
   "source": [
    "print(encoder_input[:3])\n",
    "print(decoder_input[:3])\n",
    "print(decoder_target[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'salut'"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "fra_tokenizer.index_word[1019]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 데이터의 크기(shape) : (33000, 8)\n",
      "프랑스어 입력데이터의 크기(shape) : (33000, 16)\n",
      "프랑스어 출력데이터의 크기(shape) : (33000, 16)\n"
     ]
    }
   ],
   "source": [
    "encoder_input = pad_sequences(encoder_input, maxlen = max_eng_seq_len, padding='post')\n",
    "decoder_input = pad_sequences(decoder_input, maxlen = max_fra_seq_len, padding='post')\n",
    "decoder_target = pad_sequences(decoder_target, maxlen = max_fra_seq_len, padding='post')\n",
    "print('영어 데이터의 크기(shape) :',np.shape(encoder_input))\n",
    "print('프랑스어 입력데이터의 크기(shape) :',np.shape(decoder_input))\n",
    "print('프랑스어 출력데이터의 크기(shape) :',np.shape(decoder_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[30  0  0  0  0  0  0  0]\n"
     ]
    }
   ],
   "source": [
    "print(encoder_input[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 학습데이터의 크기(shape) : (30000, 8)\n",
      "프랑스어 학습 입력데이터의 크기(shape) : (30000, 16)\n",
      "프랑스어 학습 출력데이터의 크기(shape) : (30000, 16)\n"
     ]
    }
   ],
   "source": [
    "n_of_val = 3000\n",
    "\n",
    "encoder_input_train = encoder_input[:-n_of_val]\n",
    "decoder_input_train = decoder_input[:-n_of_val]\n",
    "decoder_target_train = decoder_target[:-n_of_val]\n",
    "\n",
    "encoder_input_test = encoder_input[-n_of_val:]\n",
    "decoder_input_test = decoder_input[-n_of_val:]\n",
    "decoder_target_test = decoder_target[-n_of_val:]\n",
    "\n",
    "print('영어 학습데이터의 크기(shape) :',np.shape(encoder_input_train))\n",
    "print('프랑스어 학습 입력데이터의 크기(shape) :',np.shape(decoder_input_train))\n",
    "print('프랑스어 학습 출력데이터의 크기(shape) :',np.shape(decoder_target_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30000, 8)\n",
      "(30000, 16)\n",
      "(30000, 16)\n",
      "(3000, 8)\n",
      "(3000, 16)\n",
      "(3000, 16)\n"
     ]
    }
   ],
   "source": [
    "print(encoder_input_train.shape)\n",
    "print(decoder_input_train.shape)\n",
    "print(decoder_target_train.shape)\n",
    "print(encoder_input_test.shape)\n",
    "print(decoder_input_test.shape)\n",
    "print(decoder_target_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Masking\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "embedding_size = 50\n",
    "hidden_size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인코더에서 사용할 임베딩 층\n",
    "encoder_inputs = Input(shape=(None, ))\n",
    "enc_emb =  Embedding(eng_vocab_size, embedding_size,\n",
    "                    input_length=max_eng_seq_len)(encoder_inputs)\n",
    "enc_masking = Masking(mask_value=0.0)(enc_emb)\n",
    "encoder_lstm = LSTM(hidden_size, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(enc_masking)\n",
    "encoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_inputs = Input(shape=(None, ), name='decoder_input')\n",
    "dec_emb =  Embedding(fra_vocab_size, embedding_size)(decoder_inputs)\n",
    "dec_masking = Masking(mask_value=0.0)(dec_emb)\n",
    "decoder_lstm = LSTM(hidden_size, return_sequences = True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(dec_masking, initial_state = encoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_softmax_layer = Dense(fra_vocab_size, activation='softmax')\n",
    "decoder_outputs = decoder_softmax_layer(decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "decoder_input (InputLayer)      [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, None, 50)     233200      input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)         (None, None, 50)     401950      decoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "masking (Masking)               (None, None, 50)     0           embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "masking_1 (Masking)             (None, None, 50)     0           embedding_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   [(None, 50), (None,  20200       masking[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                   [(None, None, 50), ( 20200       masking_1[0][0]                  \n",
      "                                                                 lstm_2[0][1]                     \n",
      "                                                                 lstm_2[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, None, 8039)   409989      lstm_3[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 1,085,539\n",
      "Trainable params: 1,085,539\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "938/938 [==============================] - 12s 13ms/step - loss: 2.0211 - val_loss: 1.7794\n",
      "Epoch 2/15\n",
      "938/938 [==============================] - 10s 11ms/step - loss: 1.4029 - val_loss: 1.6122\n",
      "Epoch 3/15\n",
      "938/938 [==============================] - 10s 11ms/step - loss: 1.2780 - val_loss: 1.5214\n",
      "Epoch 4/15\n",
      "938/938 [==============================] - 11s 12ms/step - loss: 1.2004 - val_loss: 1.4588\n",
      "Epoch 5/15\n",
      "938/938 [==============================] - 10s 11ms/step - loss: 1.1402 - val_loss: 1.4091\n",
      "Epoch 6/15\n",
      "938/938 [==============================] - 10s 11ms/step - loss: 1.0914 - val_loss: 1.3612\n",
      "Epoch 7/15\n",
      "938/938 [==============================] - 11s 12ms/step - loss: 1.0518 - val_loss: 1.3320\n",
      "Epoch 8/15\n",
      "938/938 [==============================] - 11s 11ms/step - loss: 1.0211 - val_loss: 1.3090\n",
      "Epoch 9/15\n",
      "938/938 [==============================] - 11s 12ms/step - loss: 1.0019 - val_loss: 1.2968\n",
      "Epoch 10/15\n",
      "938/938 [==============================] - 11s 12ms/step - loss: 0.9867 - val_loss: 1.2894\n",
      "Epoch 11/15\n",
      "938/938 [==============================] - 11s 12ms/step - loss: 0.9758 - val_loss: 1.2804\n",
      "Epoch 12/15\n",
      "938/938 [==============================] - 10s 11ms/step - loss: 0.9749 - val_loss: 1.2892\n",
      "Epoch 13/15\n",
      "938/938 [==============================] - 11s 12ms/step - loss: 0.9747 - val_loss: 1.2880\n",
      "Epoch 14/15\n",
      "938/938 [==============================] - 10s 11ms/step - loss: 0.9576 - val_loss: 1.2596\n",
      "Epoch 15/15\n",
      "938/938 [==============================] - 10s 11ms/step - loss: 0.9362 - val_loss: 1.2612\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f2fa8a83690>"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=[encoder_input_train, decoder_input_train], \n",
    "          y=decoder_target_train, \n",
    "          validation_data = ([encoder_input_test, decoder_input_test], \n",
    "                             decoder_target_test),\n",
    "          batch_size=32, \n",
    "          epochs=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         [(None, None)]            0         \n",
      "_________________________________________________________________\n",
      "embedding_3 (Embedding)      (None, None, 50)          233200    \n",
      "_________________________________________________________________\n",
      "masking (Masking)            (None, None, 50)          0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                [(None, 50), (None, 50),  20200     \n",
      "=================================================================\n",
      "Total params: 253,400\n",
      "Trainable params: 253,400\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder_model = Model(inputs = encoder_inputs, outputs = encoder_states)\n",
    "encoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이전 time step의 hidden state를 저장하는 텐서\n",
    "decoder_state_input_h = Input(shape=(hidden_size,))\n",
    "# 이전 time step의 cell state를 저장하는 텐서\n",
    "decoder_state_input_c = Input(shape=(hidden_size,))\n",
    "# 이전 time step의 hidden state와 cell state를 하나의 변수에 저장\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "# decoder_states_inputs를 현재 time step의 초기 상태로 사용.\n",
    "# 구체적인 동작 자체는 def decode_sequence()에 구현.\n",
    "dec_emb2 = Embedding(fra_vocab_size, embedding_size)(decoder_inputs)\n",
    "dec_masking2 = Masking(mask_value=0.0)(dec_emb2)\n",
    "\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_masking2, initial_state = decoder_states_inputs)\n",
    "decoder_states2 = [state_h2, state_c2]\n",
    "decoder_outputs2 = decoder_softmax_layer(decoder_outputs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_10\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "decoder_input (InputLayer)      [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_7 (Embedding)         (None, None, 50)     401950      decoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "masking_3 (Masking)             (None, None, 50)     0           embedding_7[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "input_9 (InputLayer)            [(None, 50)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_10 (InputLayer)           [(None, 50)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                   [(None, None, 50), ( 20200       masking_3[0][0]                  \n",
      "                                                                 input_9[0][0]                    \n",
      "                                                                 input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, None, 8039)   409989      lstm_3[3][0]                     \n",
      "==================================================================================================\n",
      "Total params: 832,139\n",
      "Trainable params: 832,139\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "decoder_model = Model(inputs=[decoder_inputs] + decoder_states_inputs, outputs=[decoder_outputs2] + decoder_states2)\n",
    "decoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng2idx = eng_tokenizer.word_index\n",
    "fra2idx = fra_tokenizer.word_index\n",
    "idx2eng = eng_tokenizer.index_word\n",
    "idx2fra = fra_tokenizer.index_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # 입력으로부터 인코더의 상태를 얻음\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # <start>에 해당하는 원-핫 벡터 생성\n",
    "    target_seq = np.zeros((1,1))\n",
    "    target_seq[0, 0] = fra2idx['<start>']\n",
    "    \n",
    "    stop_condition = False\n",
    "    decoded_sentence = \"\"\n",
    "\n",
    "    # stop_condition이 True가 될 때까지 루프 반복\n",
    "    while not stop_condition:\n",
    "        # 이점 시점의 상태 states_value를 현 시점의 초기 상태로 사용\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "        # 예측 결과를 문자로 변환\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = idx2fra[sampled_token_index]\n",
    "\n",
    "        # 현재 시점의 예측 문자를 예측 문장에 추가\n",
    "        decoded_sentence += ' '+sampled_char\n",
    "\n",
    "        # <eos>에 도달하거나 최대 길이를 넘으면 중단.\n",
    "        if (sampled_char == '<end>' or\n",
    "           len(decoded_sentence) > max_fra_seq_len):\n",
    "            stop_condition = True\n",
    "\n",
    "        # 현재 시점의 예측 결과를 다음 시점의 입력으로 사용하기 위해 저장\n",
    "        target_seq = np.zeros((1, 1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # 현재 시점의 상태를 다음 시점의 상태로 사용하기 위해 저장\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "입력 문장: run !\n",
      "정답 문장: cours !\n",
      "번역기가 번역한 문장:  je ne . . . . . \n",
      "-----------------------------------\n",
      "입력 문장: i left .\n",
      "정답 문장: je suis partie .\n",
      "번역기가 번역한 문장:  j maintenant de \n",
      "-----------------------------------\n",
      "입력 문장: call us .\n",
      "정답 문장: appelez nous !\n",
      "번역기가 번역한 문장:  je me . . . . . \n",
      "-----------------------------------\n",
      "입력 문장: how nice !\n",
      "정답 문장: comme c est gentil !\n",
      "번역기가 번역한 문장:  j me ete . . . \n",
      "-----------------------------------\n",
      "입력 문장: turn left .\n",
      "정답 문장: tourne a gauche .\n",
      "번역기가 번역한 문장:  c fait . . . . \n"
     ]
    }
   ],
   "source": [
    "for seq_index in [3,50,100,300,1001]: # 입력 문장의 인덱스 (자유롭게 선택해 보세요)\n",
    "    input_seq = encoder_input_test[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print(35 * \"-\")\n",
    "    print('입력 문장:', lines.eng[seq_index])\n",
    "    print('정답 문장:', lines.fra[seq_index][8:len(lines.fra[seq_index])-6]) # '\\t'와 '\\n'을 빼고 출력\n",
    "    print('번역기가 번역한 문장:', decoded_sentence[:len(decoded_sentence)-1]) # '\\n'을 빼고 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 느낀점\n",
    "1. 훈련할 때와 테스트 할 때의 단계가 달라서, 모델이 새로 필요하다는 것은 이해했다 그러나 모델을 연결 시키고, 조건을 만족할 때까지 model.predict를 계속하는 방식이 생소한 것 같다.\n",
    "2. 번역기 성능이 매우 나쁘다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
